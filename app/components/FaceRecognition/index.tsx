'use client';

import { useEffect, useRef } from 'react';
import * as faceapi from 'face-api.js';

// ì–¼êµ´ ë°ì´í„° ë¡œì»¬ ìŠ¤í† ë¦¬ì§€ ì €ì¥ (ìµœëŒ€ 30ê°œ ìœ ì§€)
const saveFaceDataToLocalStorage = (descriptor: Float32Array) => {
  const storedData = localStorage.getItem('faceData');
  const faceList = storedData ? JSON.parse(storedData) : [];

  // ìµœëŒ€ 30ê°œ ìœ ì§€: ì´ˆê³¼ ì‹œ ì˜¤ë˜ëœ ë°ì´í„° ì‚­ì œ
  if (faceList.length >= 30) {
    faceList.shift(); // ê°€ì¥ ì˜¤ë˜ëœ ë°ì´í„° ì œê±°
    console.log('ğŸ—‘ï¸ ì˜¤ë˜ëœ ë°ì´í„° ì‚­ì œ');
  }

  faceList.push({ descriptor: Array.from(descriptor), timestamp: Date.now() });
  localStorage.setItem('faceData', JSON.stringify(faceList));
  console.log('âœ… ì–¼êµ´ ë°ì´í„°ê°€ LocalStorageì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!');
};

export default function FaceRecognition() {
  const videoRef = useRef<HTMLVideoElement>(null);
  const canvasRef = useRef<HTMLCanvasElement>(null);

  useEffect(() => {
    const loadModels = async () => {
      try {
        await faceapi.nets.tinyFaceDetector.loadFromUri('/models');
        await faceapi.nets.faceLandmark68Net.loadFromUri('/models');
        await faceapi.nets.faceRecognitionNet.loadFromUri('/models');
        console.log('âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ');
      } catch (error) {
        console.error('âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨:', error);
      }
    };
    loadModels();
  }, []);

  const handleFaceRegister = async () => {
    if (!videoRef.current || !canvasRef.current) return;

    try {
      const stream = await navigator.mediaDevices.getUserMedia({ video: true });
      videoRef.current.srcObject = stream;
      console.log('âœ… ì›¹ìº  ì‹¤í–‰ ì„±ê³µ');

      videoRef.current.onloadedmetadata = () => {
        console.log('ğŸ“¸ ë¹„ë””ì˜¤ ë¡œë”© ì™„ë£Œ, ì–¼êµ´ ê°ì§€ ì‹œì‘...');
        detectFace();
      };
    } catch (err) {
      console.error('âŒ ì›¹ìº  ì ‘ê·¼ ì˜¤ë¥˜:', err);
    }
  };

  const detectFace = async () => {
    if (!videoRef.current || !canvasRef.current) return;

    const canvas = canvasRef.current;
    const video = videoRef.current;
    const displaySize = {
      width: video.videoWidth,
      height: video.videoHeight,
    };

    faceapi.matchDimensions(canvas, displaySize);

    setInterval(async () => {
      const detections = await faceapi
        .detectSingleFace(video, new faceapi.TinyFaceDetectorOptions())
        .withFaceLandmarks()
        .withFaceDescriptor();

      const ctx = canvas.getContext('2d');
      if (!ctx) return;
      ctx.clearRect(0, 0, canvas.width, canvas.height);

      if (detections) {
        const resizedDetections = faceapi.resizeResults(detections, displaySize);
        faceapi.draw.drawDetections(canvas, resizedDetections);
        faceapi.draw.drawFaceLandmarks(canvas, resizedDetections);
        console.log('ğŸ§ ì–¼êµ´ ê°ì§€ë¨:', detections.descriptor);

        saveFaceDataToLocalStorage(detections.descriptor);
        console.log('âœ… ì €ì¥ ì™„ë£Œ!');
      }
      stopCamera();
    }, 200);
  };

  const stopCamera = () => {
    if (videoRef.current?.srcObject) {
      const stream = videoRef.current.srcObject as MediaStream;
      stream.getTracks().forEach((track) => track.stop());
      videoRef.current.srcObject = null;
      console.log('ğŸ“´ ì›¹ìº  ì¢…ë£Œ');
    }
  };

  return (
    <div className="relative flex flex-col items-center">
      <video ref={videoRef} autoPlay muted className="border rounded w-80 h-60" />
      <canvas ref={canvasRef} className="absolute top-0 w-80 h-60" />
      <button onClick={handleFaceRegister} className="mt-2 p-2 bg-blue-500 text-white rounded">
        ì–¼êµ´ ë“±ë¡í•˜ê¸°
      </button>
    </div>
  );
}
